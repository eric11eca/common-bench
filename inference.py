import random
import torch
import wandb
import logging
import openai
import numpy as np

from tqdm import tqdm
from pathlib import Path
from pprint import pprint

from transformers import StoppingCriteria, StoppingCriteriaList
from transformers import AutoTokenizer, AutoConfig
from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM

from common_bench.dataset import CommonDataset
from common_bench.model import TranslationOutput
from common_bench.utils.py_io import *


openai.api_key = key3
openai.organization = org2

util_logger = logging.getLogger(
    'common_bench.inference'
)

model_path_hf = {
    "flan-t5": ("google/flan-t5-xxl", "chenz16/flan-xxl-sharded-fp16"),
    "t0pp": ("bigscience/T0pp", "chenz16/T0pp-11b-sharded-fp16"),
    "unified-qa": ("allenai/unifiedqa-v2-t5-11b-1251000", "chenz16/unifiedqa-11b-sharded-fp16"),
    "gptj": ("EleutherAI/gpt-j-6B", "sharded-gpt-j-6B"),
    "macaw-11b": ("allenai/macaw-11b", "chenz16/macaw-11b-sharded-fp16"),
    "bloom-3b": ("bigscience/bloom-3b", "sharded-bloom-3b"),
    'opt-66b': ('facebook/opt-66b', '/nlpdata1/home/sooh/opt66/')
}

model_class_registry = {
    "t5": AutoModelForSeq2SeqLM,
    "opt": AutoModelForCausalLM,
    "bloom": AutoModelForCausalLM,
    "gpt": AutoModelForCausalLM
}


def init_wandb(args):
    runner = wandb.init(
        project=args.wandb_project,
        entity=args.wandb_entity,
        name=args.wandb_name
    )
    return runner


def load_model(model_name, local_name, model_class):
    """Function responsible for loading the model for the model runner.

    :param args: the arguments for the model runner
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    config = AutoConfig.from_pretrained(model_name)

    model = model_class.from_pretrained(
        local_name,
        local_files_only=False,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        offload_folder="offload",
        offload_state_dict=True
    )
    model = model.eval()
    return model, tokenizer, config


def load_data(args, tokenizer):
    """Function responsible for loading the data for the model runner.

    :param args: the arguments for the model runner
    :param tokenizer: the tokenizer for the model runner
    """
    if args.do_icl:
        train_data = CommonDataset(
            util_logger,
            args,
            tokenizer,
            args.data_dir,
            data_type="train",
            is_training=False,
            ic_examples=[]
        )

        if args.search:
            example_data = read_jsonl(
                f"./data/{args.dataset}/train_{args.encoder}.jsonl")

            train_dict = {}
            for instance in train_data.data:
                key = instance["guid"]
                train_dict[key] = instance

            ic_examples = {}
            for instance in example_data:
                key = instance["guid"]
                ic_examples[key] = [
                    train_dict[id] for id in instance["examples"]]
                assert len(ic_examples[key]) == 16
                ic_examples[key] = ic_examples[key][:args.num_examples]
        else:
            if args.dataset == "com2sense":
                true_examples, neg_examples = [], []
                for instance in train_data.data:
                    if instance["answer"] == "Yes":
                        true_examples.append(instance)
                    else:
                        neg_examples.append(instance)
                ic_examples = random.choices(true_examples, k=args.num_examples // 2) + \
                    random.choices(neg_examples, k=args.num_examples // 2)
            else:
                ic_examples = random.choices(
                    train_data.data, k=args.num_examples)
    else:
        ic_examples = []

    test_data = CommonDataset(
        util_logger,
        args,
        tokenizer,
        args.data_dir,
        data_type="test",
        is_training=False,
        ic_examples=ic_examples
    )

    dataloader = test_data.load_dataloader()
    return dataloader


def output_parser_metrics(args, raw_output):
    """Function responsible for parsing the raw_output and computing particular
    metrics from the model runner output.

    :param args: the arguments for the model runner
    :param raw_output: the raw output created by the model runner
    :rtype: tuple
    """
    metrics = {}
    sout = TranslationOutput.from_output(vars(args), raw_output)
    scores = sout.compute_metrics()
    metrics.update(scores)
    return (sout, metrics)


def evaluate_output(args, output, wandb_runner, out_file=None, metric_file=None):
    """Method for generating output produced during training and/or evaluation.

    :param args: the arguments for the model runner
    :param output: the output generated by runner
    :param wandb_runner: the wandb runner for logging
    :param out_file: the file to write the output to
    :param metric_file: the file to write the metrics to
    :return: the metrics
    """
    sout, metrics = output_parser_metrics(args, output)
    if out_file:
        out_dir = Path(out_file).parent
        out_dir.mkdir(parents=True, exist_ok=True)
        outputs = []
        for instance in sout:
            outputs.append(instance)
        write_json(outputs, out_file)

        artifact = wandb.Artifact(f"test_eval_out", type='dataset')
        artifact.add_file(out_file)
        wandb_runner.log_artifact(artifact)

    if metric_file:
        out_dir = Path(metric_file).parent
        out_dir.mkdir(parents=True, exist_ok=True)
        write_json(metrics, metric_file)

        artifact = wandb.Artifact(f"test_metrics", type='dataset')
        artifact.add_file(metric_file)
        wandb_runner.log_artifact(artifact)

    return metrics


def parse_checkpoint_path(args):
    model_class = model_class_registry[args.model_type]
    hf_name = model_path_hf[args.model_name_or_path]
    if isinstance(hf_name, tuple):
        model_name = hf_name[0]
        local_name = hf_name[1]
    else:
        model_name = hf_name
        local_name = hf_name
    return model_name, local_name, model_class


class StopTokenCriteria(StoppingCriteria):
    """
    Stop when a stop token is generated.

    :param stop_tokens_ids: the list of stop token ids
    """

    def __init__(self, stop_tokens_ids: list):
        self.stop_tokens_ids = stop_tokens_ids

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        return np.any([x in input_ids for x in self.stop_tokens_ids])


class Text2Generator():
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.tokenizer.pad_token = tokenizer.eos_token

        stop_tokens = ['\n\n']
        self.stop_condition = StopTokenCriteria(
            [tokenizer.encode(
                token, add_special_tokens=False
            )[0] for token in stop_tokens]
        )

    def generate(self, print_out, **generate_kwargs):
        device = torch.cuda.current_device()

        output_length = 0
        answers = [data for data in print_out['answer']]
        for answer in answers:
            out_ids = self.tokenizer(answer, return_tensors="pt").input_ids
            output_length = max(output_length, out_ids.size(1))

        input_length = 0
        questions = [data for data in print_out['question']]
        for question in questions:
            input_ids = self.tokenizer(question, return_tensors="pt").input_ids
            input_length = max(input_length, input_ids.size(1))

        input_ids = self.tokenizer(
            questions,
            padding=True,
            truncation=True,
            max_length=input_length,
            return_tensors="pt"
        ).input_ids.to(device)

        greedy_outputs = self.model.generate(
            input_ids.to(device),
            max_new_tokens=32,
            stopping_criteria=StoppingCriteriaList([self.stop_condition]),
            **generate_kwargs
        )

        outputs = self.tokenizer.batch_decode(
            greedy_outputs,
            skip_special_tokens=True
        )

        clean_outputs = [gen.replace(q, "")
                         for q, gen in zip(questions, outputs)]

        return clean_outputs


def query_gpt3(question):
    response = openai.Completion.create(
        model="text-davinci-002",
        prompt=question,
        temperature=0,
        max_tokens=64,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )

    return response.choices[0].text.strip().replace("\n", "")


def run_acclerate(args):
    torch.set_grad_enabled(False)

    out_file_name = f"test_eval_out.json"
    metirc_file_name = f"test_metrics.json"
    out_file_pth = f"{args.run_dir}/{out_file_name}"
    metric_file_pth = f"{args.run_dir}/{metirc_file_name}"
    tmp_out_file_pth = f"{args.run_dir}/tmp_{out_file_name}"

    if args.model_name_or_path == 't5':
        gen_kwargs = {
            "num_beams": 10,
            "do_sample": False,
            "num_return_sequences": 1,
        }
    else:
        gen_kwargs = {
            "num_beams": 10,
            "temperature": 0.0,
            "top_p": 1.0,
            "num_return_sequences": 1,
        }

    output_all = []
    if not args.model_name_or_path == 'gpt3':
        model_name, local_name, model_class = parse_checkpoint_path(args)
        model, tokenizer, _ = load_model(model_name, local_name, model_class)

        if args.model_type != "t5" and args.model_type == "gpt":
            model.config.pad_token_id = tokenizer.eos_token_id
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.padding_side = 'left'

        tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom")
        dataloader = load_data(args, tokenizer)
        generator = Text2Generator(model, tokenizer)

        for batch in tqdm(dataloader):
            print_out = batch["print_out"]
            pipe_out = generator.generate(
                print_out,
                **gen_kwargs
            )
            print_out["gen_out"] = pipe_out
            output_all.append({"print_out": print_out})
    else:
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token
        dataloader = load_data(args, tokenizer)

        output_all = []
        for batch in tqdm(dataloader):
            print_out = batch["print_out"]
            pipe_out = [query_gpt3(q) for q in print_out["question"]]
            print_out["gen_out"] = pipe_out
            output_all.append({"print_out": print_out})
            write_jsonl(output_all, tmp_out_file_pth)

        # output_all = read_jsonl(
        #     "./output/20221221-173613/tmp_test_eval_out.json")

    wandb_runner = init_wandb(args)
    metrics_out = evaluate_output(
        args,
        output_all,
        wandb_runner,
        out_file_pth,
        metric_file_pth
    )

    wandb_runner.log(metrics_out)
    print("Inference Finished ==== Metrics: ")
    pprint(metrics_out)
    wandb_runner.finish()
